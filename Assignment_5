The purpose of this assignment is to implement Hierarchical Clustering
---
title: "Cereals Hierarchical Clustering"
output:
  html_document:
    df_print: paged
---
```{r}
library(stats)
library(cluster)

df_cereals <- read.csv("C:/Users/AMAMBA/Downloads/MSBA/RStudio/datasets/Cereals.csv")
 
dim(df_cereals) # Examine the dimension of the data set

# The data set has 77 rows and 16 columns

 
head(df_cereals)
 
summary(df_cereals)

```

From the summary we can see that there are some missing data in three columns: carbo, sugars, potass

```{r}
cereals_treated <-  na.omit(df_cereals) #Remove any missing values 

dim(cereals_treated) # Re-examine the dimension of the data set

```
It looks like we lost 3 rows of data by removing the missing values. The data set was reduced from 77 x 16 to 74 x 16

```{r}
# Select the numeric data for scaling

cereals_numeric <- cereals_treated[,4:16]


# Scale/standardize the data
cereals_scaled <- scale(cereals_numeric)

# Examine the scaled data
head(cereals_scaled)

```

```{r}
# Dissimilarity Matrix
d <- dist(cereals_scaled, method = "euclidean")

# Hierarchical clustering using the complete linkage
hc1 <- hclust(d, method = "complete")

# Plot the obtained dendogram
plot(hc1, cex = 0.5, hang = -1)

```

```{r}
# Using AGNES Method to Cluster the cereals data

cereals_agnes <- cereals_scaled

# Compute the various linkage method that could be used to determine the agnes 

# This computes the single linkage method used to determine the agnes
hc_cereal_single <- agnes(cereals_agnes, method = "single")

hc_cereal_single$ac

# This computes the average linkage method used to determine the agnes
hc_cereal_average <- agnes(cereals_agnes, method = "average")

hc_cereal_average$ac

# This computes the complete linkage method used to determine the agnes
hc_cereal_complete <- agnes(cereals_agnes, method = "complete")

hc_cereal_complete$ac


# This computes the Ward linkage method used to determine the agnes
hc_cereal_ward <- agnes(cereals_agnes, method = "ward")

hc_cereal_ward$ac


# hc_cereal_ward$ac provides the best linkage method.

```

```{r}
# Plot the obtained dendogram
pltree(hc_cereal_ward, cex = 0.5, hang = -1, main = "Dendrogram of AGNES")

```

The choice of number of clusters:  
To determine the optimal number of clusters, we look for the largest gap between the merges at different heights. This is usually where a horizontal line can be drawn across the dendrogram without intersecting any of the vertical lines that represent merges between clusters.

Given that the height ranges from 0 to 17, and considering the merge points above, drawing a line at height 16 would result in three distinct clusters. It then would seem that choosing three clusters could be a reasonable decision for the AGNES dendrogram


Below is a comparison of the two dendrograms ploted above:

Methodology: The first dendrogram is generated by using the “complete” linkage method, while the second uses the  “ward”  method. The “ward” method minimizes the total within-cluster variance, and the “complete” method considers the maximum distance between clusters for merging.

Scale: The scale of ‘Height’ in the first dendrogram ranges from 0 to 12, whereas the second ranges from 0 to 17. This indicates that the first dendrogram represents a smaller range of distances or dissimilarities between clusters.

Structure: Both dendrograms show the hierarchical clustering structure, but the points at which clusters are combined differ due to the different methods used. The AGNES dendrogram appears to have a more balanced structure, while the one done with the complete linkage has clusters combining at lower heights which suggests that there are more similarity within clusters early on.


The code below will demonstrate how to cut an AGNES dendrogram at a specific height. We will use the cutree function from the stats package, which is available by default in R.

```{r}
# Recall that 'hc_cereal_ward' is the AGNES object from which the dendrogram was created


# Convert AGNES object to hclust object
hclust_obj <- as.hclust(hc_cereal_ward)

# Cut the dendrogram at height 16
cut_clusters <- cutree(hclust_obj, h = 16)

# Print the resulting clusters
print(cut_clusters)


# Plot the dendrogram and show the line of partition of the clusters 
plot(hclust_obj, cex = 0.5, hang = -1, main = "Cut Dendrogram of AGNES with Line of Clusters Partition", xlab = "Clusters", sub = paste("Cut at height =", 16))

# Add a horizontal line at height 16 to show the cut
abline(h = 16, col = "red", lty = 2)

```


To check for the stability of the structure
```{r}

# Step 1: Create a cluster partition A
set.seed(103) # for reproducibility
partition_A <- kmeans(cereals_scaled, centers = 3)

# Step 2: Use the cluster centroids from A to assign each record in partition B

# let's assume partition B is the second half of the cereals dataset
partition_B <- cereals_scaled[(nrow(cereals_scaled)/2 + 1):nrow(cereals_scaled), ]

# Assign each record in partition B to the cluster with the closest centroid
partition_B_clusters <- apply(partition_B, 1, function(x) {
  # Compute the Euclidean distance between the record and each cluster centroid

distances <- sqrt(rowSums(sweep(partition_A$centers, 2, x, "-")^2))
  
  # Return the cluster with the smallest distance
  return(which.min(distances))
})

# Step 3: Assess how consistent the cluster assignments are compared to the assignments based on all the data
# For this, we can compare the cluster assignments of partition B with those of the corresponding records in partition A
partition_A_clusters <- partition_A$cluster[(nrow(cereals_numeric)/2 + 1):nrow(cereals_numeric)]

# Compute the consistency as the proportion of records in partition B that were assigned to the same cluster as in partition A
consistency <- sum(partition_B_clusters == partition_A_clusters) / length(partition_B_clusters)

print(paste("Consistency: ", consistency))



```
The consistency of the cluster assignments between partition A and B is shown above as 1. 
The closer the consistency is to 1, the more consistent the cluster assignments are.





The elementary public schools would like to choose a set of cereals to include in their daily cafeterias. Every day a different cereal is offered, but all cereals should support a healthy diet. For this goal, you are requested to find a cluster of “healthy cereals.” Should the data be normalized? If not, how should they be used in the cluster analysis?

Yes, normalizing the data is generally recommended before performing cluster analysis, especially when the features (or variables) are measured in different units or scales. This is because clustering algorithms, like AGNES, typically use a measure of distance (like Euclidean distance) between data points to form clusters. If the features are not on the same scale, one feature may dominate the distance measure simply because of its scale, not because it necessarily carries more information.

For example, take three features called (a) “calories” which ranges from 0 to 160, (b) "sodium" which ranges from 0 to 320, the “potass” feature which ranges from 0 to 330, and the the “vitamins” feature which ranges from 0 to 100. These features will dominate the distance measure because of their larger scale. Normalizing these features will bring them to the same scale as the other features that have smaller ranges and will allow each to contribute more equally to the distance measure.

In this context, we want to choose healthy cereals from features like “calories”, “protein”, “fat”, “fiber”, “sugar”, etc. These are likely measured in different units and scales, so normalizing them would be appropriate before performing cluster analysis.

After normalization, we can perform a cluster analysis to group the cereals based on their nutritional profiles. The “healthy” cluster(s) would be those with low calories, sugar, and fat, and high fiber, vitamins, protein, etc., depending on how you define “healthy”. We can then choose cereals from these “healthy” cluster(s) to include in the daily cafeteria offerings
